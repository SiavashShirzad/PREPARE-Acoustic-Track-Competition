{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import opensmile\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from transformers import WhisperFeatureExtractor, WhisperModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "## change for training/ testing feature extraction\n",
    "dataType = 'train'\n",
    "path = \"\"\n",
    "audio_folder = path + dataType + \"_audios\"\n",
    "original_sampling_rate = 48000  \n",
    "target_sampling_rate = 16000    \n",
    "\n",
    "def load_and_resample_audio(file_path, target_sampling_rate):\n",
    "    waveform, original_rate = torchaudio.load(file_path)\n",
    "    original_length = waveform.shape[-1] / original_rate\n",
    "    \n",
    "    if original_rate != target_sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=target_sampling_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze(), original_length\n",
    "\n",
    "# Generate model for embedding extraction using OpenSMILE\n",
    "def audio_embeddings_model(model_name):\n",
    "    if model_name == \"compare\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    elif model_name == \"egemaps\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Embedding extraction function for OpenSMILE features\n",
    "def audio_embeddings(audio_list, model, sampling_rate):\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=\"Extracting OpenSMILE features\"):\n",
    "        embeddings = model.process_signal(audio.numpy(), sampling_rate)\n",
    "        embeddings_list.append(embeddings.values.flatten())\n",
    "    return embeddings_list\n",
    "\n",
    "# Generic function to extract embeddings from a Transformer-based audio model\n",
    "def extract_embeddings_from_transformer_model(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name}...\")\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        inputs = feature_extractor(audio.numpy(), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract encoder embeddings from Whisper\n",
    "def extract_whisper_embeddings(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name} (Whisper) ...\")\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "    model = WhisperModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        audio_np = audio.numpy()\n",
    "        inputs = feature_extractor(audio_np, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.encoder(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract additional features\n",
    "def extract_additional_features(audio, sampling_rate):\n",
    "    audio = audio.numpy()\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sampling_rate)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sampling_rate)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate, roll_percent=0.85)\n",
    "    zero_crossings = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sampling_rate)\n",
    "    pitch = np.max(pitches, axis=0)\n",
    "    features = np.hstack([\n",
    "        np.mean(mfccs, axis=1),\n",
    "        np.mean(chroma, axis=1),\n",
    "        np.mean(spectral_centroid, axis=1),\n",
    "        np.mean(spectral_rolloff, axis=1),\n",
    "        np.mean(zero_crossings, axis=1),\n",
    "        np.mean(pitch)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "\n",
    "# Load and process MP3 files\n",
    "audio_files = [os.path.join(audio_folder, file) for file in os.listdir(audio_folder) if file.endswith(\".mp3\")]\n",
    "\n",
    "data = [load_and_resample_audio(file, target_sampling_rate) for file in tqdm(audio_files)]\n",
    "audio_list = [item[0] for item in data]\n",
    "audio_lengths = [item[1] for item in data]\n",
    "egemaps_model = audio_embeddings_model(\"egemaps\")\n",
    "print(\"Extracting eGeMAPS\")\n",
    "egemaps_features = audio_embeddings(audio_list, egemaps_model, target_sampling_rate)\n",
    "wav2vec2_model_name = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "whisper_model_name = \"openai/whisper-medium\"\n",
    "print(\"Extracting Wav2Vec2 \")\n",
    "wav2vec2_features = extract_embeddings_from_transformer_model(audio_list, wav2vec2_model_name, target_sampling_rate)\n",
    "print(\"Extracting Whisper\")\n",
    "whisper_features = extract_whisper_embeddings(audio_list, whisper_model_name, target_sampling_rate)\n",
    "\n",
    "additional_features_list = []\n",
    "for audio in tqdm(audio_list, desc=\"Extracting additional features\"):\n",
    "    additional_features = extract_additional_features(audio, target_sampling_rate)\n",
    "    additional_features_list.append(additional_features)\n",
    "\n",
    "additional_feature_columns = (\n",
    "    [f\"mfcc_{i}\" for i in range(1, 14)]\n",
    "    + [f\"chroma_{i}\" for i in range(1, 13)]\n",
    "    + [\"spectral_centroid\", \"spectral_rolloff\", \"zero_crossing_rate\", \"pitch\"]\n",
    ")\n",
    "additional_features_df = pd.DataFrame(additional_features_list, columns=additional_feature_columns)\n",
    "\n",
    "# Create DataFrame for all features\n",
    "features_df = pd.DataFrame(egemaps_features, columns=egemaps_model.column_names)\n",
    "features_df[\"Wav2Vec2_embeddings\"] = list(wav2vec2_features)\n",
    "features_df[\"Whisper_embeddings\"] = list(whisper_features)\n",
    "features_df[\"file_name\"] = [os.path.basename(file) for file in audio_files]\n",
    "features_df[\"augmentation_type\"] = \"original\"  # tag as original\n",
    "features_df = pd.concat([features_df, additional_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df['Wav2Vec2_embeddings'].tolist(), columns=[f'Embedding1_{i+1}' for i in range(len(features_df['Wav2Vec2_embeddings'][0]))])\n",
    "features_df_exctracted = pd.concat([features_df.drop(columns=['Wav2Vec2_embeddings']), embeddings_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df_exctracted['Whisper_embeddings'].tolist(), columns=[f'Embedding2_{i+1}' for i in range(len(features_df_exctracted['Whisper_embeddings'][0]))])\n",
    "features_df_exctracted = pd.concat([features_df_exctracted.drop(columns=['Whisper_embeddings']), embeddings_features_df], axis=1)\n",
    "features_df_exctracted.to_csv(\"Features/\"+dataType+\"_audio_features.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
