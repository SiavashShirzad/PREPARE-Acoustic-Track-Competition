{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from transformers import WhisperFeatureExtractor, WhisperModel\n",
    "import joblib\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "from mutagen.mp3 import MP3\n",
    "import torchaudio\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import opensmile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "audio_folder = \"Path_to_audio_folder\"\n",
    "metadataPath = \"Path_to_metadata_csv_file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "dataType = 'inference'\n",
    "\n",
    "original_sampling_rate = 48000  \n",
    "target_sampling_rate = 16000    \n",
    "\n",
    "def load_and_resample_audio(file_path, target_sampling_rate):\n",
    "    waveform, original_rate = torchaudio.load(file_path)\n",
    "    original_length = waveform.shape[-1] / original_rate\n",
    "    \n",
    "    if original_rate != target_sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=target_sampling_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze(), original_length\n",
    "\n",
    "# Generate model for embedding extraction using OpenSMILE\n",
    "def audio_embeddings_model(model_name):\n",
    "    if model_name == \"compare\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    elif model_name == \"egemaps\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "# Embedding extraction function for OpenSMILE features\n",
    "def audio_embeddings(audio_list, model, sampling_rate):\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=\"Extracting OpenSMILE features\"):\n",
    "        embeddings = model.process_signal(audio.numpy(), sampling_rate)\n",
    "        embeddings_list.append(embeddings.values.flatten())\n",
    "    return embeddings_list\n",
    "\n",
    "# Generic function to extract embeddings from a Transformer-based audio model\n",
    "def extract_embeddings_from_transformer_model(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name}...\")\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        inputs = feature_extractor(audio.numpy(), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract encoder embeddings from Whisper\n",
    "def extract_whisper_embeddings(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name} (Whisper) ...\")\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "    model = WhisperModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        audio_np = audio.numpy()\n",
    "        inputs = feature_extractor(audio_np, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.encoder(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract additional features\n",
    "def extract_additional_features(audio, sampling_rate):\n",
    "    audio = audio.numpy()\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sampling_rate)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sampling_rate)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate, roll_percent=0.85)\n",
    "    zero_crossings = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sampling_rate)\n",
    "    pitch = np.max(pitches, axis=0)\n",
    "    features = np.hstack([\n",
    "        np.mean(mfccs, axis=1),\n",
    "        np.mean(chroma, axis=1),\n",
    "        np.mean(spectral_centroid, axis=1),\n",
    "        np.mean(spectral_rolloff, axis=1),\n",
    "        np.mean(zero_crossings, axis=1),\n",
    "        np.mean(pitch)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "\n",
    "# Load and process MP3 files\n",
    "audio_files = [os.path.join(audio_folder, file) for file in os.listdir(audio_folder) if file.endswith(\".mp3\")]\n",
    "\n",
    "data = [load_and_resample_audio(file, target_sampling_rate) for file in tqdm(audio_files)]\n",
    "audio_list = [item[0] for item in data]\n",
    "audio_lengths = [item[1] for item in data]\n",
    "egemaps_model = audio_embeddings_model(\"egemaps\")\n",
    "print(\"Extracting eGeMAPS\")\n",
    "egemaps_features = audio_embeddings(audio_list, egemaps_model, target_sampling_rate)\n",
    "wav2vec2_model_name = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "whisper_model_name = \"openai/whisper-medium\"\n",
    "print(\"Extracting Wav2Vec2 \")\n",
    "wav2vec2_features = extract_embeddings_from_transformer_model(audio_list, wav2vec2_model_name, target_sampling_rate)\n",
    "print(\"Extracting Whisper\")\n",
    "whisper_features = extract_whisper_embeddings(audio_list, whisper_model_name, target_sampling_rate)\n",
    "\n",
    "additional_features_list = []\n",
    "for audio in tqdm(audio_list, desc=\"Extracting additional features\"):\n",
    "    additional_features = extract_additional_features(audio, target_sampling_rate)\n",
    "    additional_features_list.append(additional_features)\n",
    "\n",
    "additional_feature_columns = (\n",
    "    [f\"mfcc_{i}\" for i in range(1, 14)]\n",
    "    + [f\"chroma_{i}\" for i in range(1, 13)]\n",
    "    + [\"spectral_centroid\", \"spectral_rolloff\", \"zero_crossing_rate\", \"pitch\"]\n",
    ")\n",
    "additional_features_df = pd.DataFrame(additional_features_list, columns=additional_feature_columns)\n",
    "\n",
    "# Create DataFrame for all features\n",
    "features_df = pd.DataFrame(egemaps_features, columns=egemaps_model.column_names)\n",
    "features_df[\"Wav2Vec2_embeddings\"] = list(wav2vec2_features)\n",
    "features_df[\"Whisper_embeddings\"] = list(whisper_features)\n",
    "features_df[\"file_name\"] = [os.path.basename(file) for file in audio_files]\n",
    "features_df[\"augmentation_type\"] = \"original\"  # tag as original\n",
    "features_df = pd.concat([features_df, additional_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df['Wav2Vec2_embeddings'].tolist(), columns=[f'Embedding1_{i+1}' for i in range(len(features_df['Wav2Vec2_embeddings'][0]))])\n",
    "features_df_exctracted = pd.concat([features_df.drop(columns=['Wav2Vec2_embeddings']), embeddings_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df_exctracted['Whisper_embeddings'].tolist(), columns=[f'Embedding2_{i+1}' for i in range(len(features_df_exctracted['Whisper_embeddings'][0]))])\n",
    "features_df_exctracted = pd.concat([features_df_exctracted.drop(columns=['Whisper_embeddings']), embeddings_features_df], axis=1)\n",
    "features_df_exctracted.to_csv(\"Features/\"+dataType+\"_audio_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('Model/model.joblib')\n",
    "TestData = pd.read_csv(\"Features/\"+dataType+\"_audio_features.csv\")\n",
    "TestData['uid'] = TestData['file_name'].apply(lambda x: x[:-4])\n",
    "meta = pd.read_csv(metadataPath)[['uid','age','gender']]\n",
    "meta['gender'] = meta['gender'].apply(lambda x : 1 if x == 'male' else 0)\n",
    "test_data = pd.merge(TestData, meta, on='uid')\n",
    "test_data.drop('augmentation_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = test_data['uid'].values\n",
    "X_test = test_data.drop(columns=['uid', 'augmentation_type', 'file_name'], errors='ignore')\n",
    "y_test_proba = model.predict_proba(X_test)\n",
    "feature_names = X_test.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df = pd.DataFrame(\n",
    "    y_test_proba,\n",
    "    columns=['diagnosis_control', 'diagnosis_mci', 'diagnosis_adrd']\n",
    ")\n",
    "proba_df['uid'] = uids\n",
    "proba_df['confidence'] = proba_df[['diagnosis_control', 'diagnosis_mci', 'diagnosis_adrd']].max(axis=1)\n",
    "final_proba_df = proba_df.loc[proba_df.groupby('uid')['confidence'].idxmax()]\n",
    "final_proba_df = final_proba_df.drop(columns=['confidence'])\n",
    "final_proba_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_proba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_proba_df.to_csv(\"inference_results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
