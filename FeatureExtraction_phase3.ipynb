{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import opensmile\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from transformers import WhisperFeatureExtractor, WhisperModel\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "## change to train/test for training/testing feature extraction\n",
    "dataType = 'test'\n",
    "path = \"/home/siavash/Downloads/FinalExplainedAcousticNIH/\"\n",
    "audio_folder = path + dataType + \"_audios\"\n",
    "original_sampling_rate = 48000  \n",
    "target_sampling_rate = 16000    \n",
    "\n",
    "def load_and_resample_audio(file_path, target_sampling_rate):\n",
    "    waveform, original_rate = torchaudio.load(file_path)\n",
    "    original_length = waveform.shape[-1] / original_rate\n",
    "    \n",
    "    if original_rate != target_sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=target_sampling_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    return waveform.squeeze(), original_length\n",
    "\n",
    "# Generate model for embedding extraction using OpenSMILE\n",
    "def audio_embeddings_model(model_name):\n",
    "    if model_name == \"compare\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    elif model_name == \"egemaps\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "def opensmile_worker(audio, model_name, sampling_rate):\n",
    "    model = audio_embeddings_model(model_name)  # Create a new model instance per process\n",
    "    embeddings = model.process_signal(audio.numpy(), sampling_rate)\n",
    "    return embeddings.values.flatten()\n",
    "\n",
    "# Parallel embedding extraction function for OpenSMILE features\n",
    "def audio_embeddings_parallel(audio_list, model_name, sampling_rate, num_processes=40):\n",
    "    worker = partial(opensmile_worker, model_name=model_name, sampling_rate=sampling_rate)\n",
    "    with Pool(num_processes) as p:\n",
    "        embeddings_list = list(tqdm(p.imap(worker, audio_list), total=len(audio_list), desc=\"Extracting OpenSMILE features\"))\n",
    "    return embeddings_list\n",
    "\n",
    "# Generic function to extract embeddings from a Transformer-based audio model\n",
    "def extract_embeddings_from_transformer_model(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name}...\")\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        inputs = feature_extractor(audio.numpy(), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract encoder embeddings from Whisper\n",
    "def extract_whisper_embeddings(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name} (Whisper) ...\")\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "    model = WhisperModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        audio_np = audio.numpy()\n",
    "        inputs = feature_extractor(audio_np, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.encoder(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract additional features\n",
    "def extract_additional_features(audio, sampling_rate):\n",
    "    audio = audio.numpy()\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sampling_rate)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sampling_rate)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate, roll_percent=0.85)\n",
    "    zero_crossings = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sampling_rate)\n",
    "    \n",
    "    if pitches.size == 0 or pitches.shape[0] == 0:\n",
    "        mean_pitch = 0.0  # Or np.nan if you prefer to flag it as missing\n",
    "    else:\n",
    "        pitch = np.max(pitches, axis=0)\n",
    "        mean_pitch = np.mean(pitch)\n",
    "    \n",
    "    features = np.hstack([\n",
    "        np.mean(mfccs, axis=1),\n",
    "        np.mean(chroma, axis=1),\n",
    "        np.mean(spectral_centroid, axis=1),\n",
    "        np.mean(spectral_rolloff, axis=1),\n",
    "        np.mean(zero_crossings, axis=1),\n",
    "        mean_pitch\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "# Parallel extraction for additional features\n",
    "def extract_additional_features_parallel(audio_list, sampling_rate, num_processes=40):\n",
    "    worker = partial(extract_additional_features, sampling_rate=sampling_rate)\n",
    "    with Pool(num_processes) as p:\n",
    "        additional_features_list = list(tqdm(p.imap(worker, audio_list), total=len(audio_list), desc=\"Extracting additional features\"))\n",
    "    return additional_features_list\n",
    "\n",
    "# Load and process MP3 files\n",
    "audio_files = [os.path.join(audio_folder, file) for file in os.listdir(audio_folder) if file.endswith(\".mp3\")]\n",
    "\n",
    "data = [load_and_resample_audio(file, target_sampling_rate) for file in tqdm(audio_files)]\n",
    "audio_list = [item[0] for item in data]\n",
    "audio_lengths = [item[1] for item in data]\n",
    "\n",
    "additional_features_list = extract_additional_features_parallel(audio_list, target_sampling_rate)\n",
    "\n",
    "additional_feature_columns = (\n",
    "    [f\"mfcc_{i}\" for i in range(1, 14)]\n",
    "    + [f\"chroma_{i}\" for i in range(1, 13)]\n",
    "    + [\"spectral_centroid\", \"spectral_rolloff\", \"zero_crossing_rate\", \"pitch\"]\n",
    ")\n",
    "additional_features_df = pd.DataFrame(additional_features_list, columns=additional_feature_columns)\n",
    "\n",
    "egemaps_model = audio_embeddings_model(\"egemaps\")\n",
    "print(\"Extracting eGeMAPS\")\n",
    "egemaps_features = audio_embeddings_parallel(audio_list, \"egemaps\", target_sampling_rate)\n",
    "wav2vec2_model_name = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "whisper_model_name = \"openai/whisper-medium\"\n",
    "print(\"Extracting Wav2Vec2 \")\n",
    "wav2vec2_features = extract_embeddings_from_transformer_model(audio_list, wav2vec2_model_name, target_sampling_rate)\n",
    "print(\"Extracting Whisper\")\n",
    "whisper_features = extract_whisper_embeddings(audio_list, whisper_model_name, target_sampling_rate)\n",
    "\n",
    "\n",
    "\n",
    "# Create DataFrame for all features\n",
    "features_df = pd.DataFrame(egemaps_features, columns=egemaps_model.column_names)\n",
    "features_df[\"Wav2Vec2_embeddings\"] = list(wav2vec2_features)\n",
    "features_df[\"Whisper_embeddings\"] = list(whisper_features)\n",
    "features_df[\"file_name\"] = [os.path.basename(file) for file in audio_files]\n",
    "features_df[\"augmentation_type\"] = \"original\"  # tag as original\n",
    "features_df = pd.concat([features_df, additional_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df['Wav2Vec2_embeddings'].tolist(), columns=[f'Embedding1_{i+1}' for i in range(len(features_df['Wav2Vec2_embeddings'][0]))])\n",
    "features_df_exctracted = pd.concat([features_df.drop(columns=['Wav2Vec2_embeddings']), embeddings_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df_exctracted['Whisper_embeddings'].tolist(), columns=[f'Embedding2_{i+1}' for i in range(len(features_df_exctracted['Whisper_embeddings'][0]))])\n",
    "features_df_exctracted = pd.concat([features_df_exctracted.drop(columns=['Whisper_embeddings']), embeddings_features_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df_exctracted.to_csv(\"Features/\"+dataType+\"_audio_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"/home/siavash/Downloads/FinalExplainedAcousticNIH/processed_30seconds.csv\")\n",
    "try:\n",
    "    data[data['sex'].notna()]['sex'] = data[data['sex'].notna()]['sex'].apply(lambda x: 'm' if x.lower()[0] == 'm' else 'f')\n",
    "except:\n",
    "    pass  \n",
    "data.loc[data['sex'].notna(), 'sex'] = data.loc[data['sex'].notna(), 'sex'].apply(lambda x: 'm' if x.lower()[0] == 'm' else 'f')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data.iloc[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torchaudio\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import opensmile\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "from transformers import WhisperFeatureExtractor, WhisperModel\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define paths\n",
    "## change to train/test for training/testing feature extraction\n",
    "dataType = 'full'\n",
    "path = \"/home/siavash/Downloads/FinalExplainedAcousticNIH/\"\n",
    "# Assume 'data' dataframe is pre-loaded with 'processed_path' column containing paths to audio files\n",
    "# audio_folder = path + dataType + \"_audios\"  # No longer needed\n",
    "original_sampling_rate = 48000  \n",
    "target_sampling_rate = 16000    \n",
    "\n",
    "def load_and_resample_audio(file_path, target_sampling_rate):\n",
    "    waveform, original_rate = torchaudio.load(file_path)\n",
    "    original_length = waveform.shape[-1] / original_rate\n",
    "    \n",
    "    if original_rate != target_sampling_rate:\n",
    "        resampler = torchaudio.transforms.Resample(orig_freq=original_rate, new_freq=target_sampling_rate)\n",
    "        waveform = resampler(waveform)\n",
    "    \n",
    "    # Average channels if multi-channel\n",
    "    if waveform.dim() > 1 and waveform.shape[0] > 1:\n",
    "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
    "    \n",
    "    return waveform.squeeze(), original_length\n",
    "\n",
    "\n",
    "\n",
    "# Generate model for embedding extraction using OpenSMILE\n",
    "def audio_embeddings_model(model_name):\n",
    "    if model_name == \"compare\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.ComParE_2016,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    elif model_name == \"egemaps\":\n",
    "        model = opensmile.Smile(\n",
    "            feature_set=opensmile.FeatureSet.eGeMAPSv02,\n",
    "            feature_level=opensmile.FeatureLevel.Functionals,\n",
    "        )\n",
    "    return model\n",
    "\n",
    "def opensmile_worker(audio, model_name, sampling_rate):\n",
    "    model = audio_embeddings_model(model_name)  # Create a new model instance per process\n",
    "    embeddings = model.process_signal(audio.numpy(), sampling_rate)\n",
    "    return embeddings.values.flatten()\n",
    "\n",
    "# Parallel embedding extraction function for OpenSMILE features\n",
    "def audio_embeddings_parallel(audio_list, model_name, sampling_rate, num_processes=40):\n",
    "    worker = partial(opensmile_worker, model_name=model_name, sampling_rate=sampling_rate)\n",
    "    with Pool(num_processes) as p:\n",
    "        embeddings_list = list(tqdm(p.imap(worker, audio_list), total=len(audio_list), desc=\"Extracting OpenSMILE features\"))\n",
    "    return embeddings_list\n",
    "\n",
    "# Generic function to extract embeddings from a Transformer-based audio model\n",
    "def extract_embeddings_from_transformer_model(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name}...\")\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(model_name)\n",
    "    model = AutoModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        inputs = feature_extractor(audio.numpy(), sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract encoder embeddings from Whisper\n",
    "def extract_whisper_embeddings(audio_list, model_name, sampling_rate):\n",
    "    print(f\"Extracting embeddings using {model_name} (Whisper) ...\")\n",
    "    feature_extractor = WhisperFeatureExtractor.from_pretrained(model_name)\n",
    "    model = WhisperModel.from_pretrained(model_name).to(device)\n",
    "    model.eval()\n",
    "\n",
    "    embeddings_list = []\n",
    "    for audio in tqdm(audio_list, desc=f\"Extracting {model_name} embeddings\"):\n",
    "        audio_np = audio.numpy()\n",
    "        inputs = feature_extractor(audio_np, sampling_rate=sampling_rate, return_tensors=\"pt\")\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model.encoder(**inputs)\n",
    "            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()\n",
    "        embeddings_list.append(embeddings.squeeze())\n",
    "    return embeddings_list\n",
    "\n",
    "# Function to extract additional features\n",
    "def extract_additional_features(audio, sampling_rate):\n",
    "    audio = audio.numpy()\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sampling_rate, n_mfcc=13)\n",
    "    chroma = librosa.feature.chroma_stft(y=audio, sr=sampling_rate)\n",
    "    spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sampling_rate)\n",
    "    spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sampling_rate, roll_percent=0.85)\n",
    "    zero_crossings = librosa.feature.zero_crossing_rate(y=audio)\n",
    "    pitches, magnitudes = librosa.core.piptrack(y=audio, sr=sampling_rate)\n",
    "    \n",
    "    if pitches.size == 0 or pitches.shape[0] == 0:\n",
    "        mean_pitch = 0.0\n",
    "    else:\n",
    "        pitch = np.max(pitches, axis=0)\n",
    "        mean_pitch = np.mean(pitch)\n",
    "    \n",
    "    # Wrap mean_pitch as 1D array\n",
    "    mean_pitch = np.array([mean_pitch])\n",
    "    \n",
    "    features = np.hstack([\n",
    "        np.mean(mfccs, axis=1),\n",
    "        np.mean(chroma, axis=1),\n",
    "        np.mean(spectral_centroid, axis=1),\n",
    "        np.mean(spectral_rolloff, axis=1),\n",
    "        np.mean(zero_crossings, axis=1),\n",
    "        mean_pitch\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "# Parallel extraction for additional features\n",
    "def extract_additional_features_parallel(audio_list, sampling_rate, num_processes=40):\n",
    "    worker = partial(extract_additional_features, sampling_rate=sampling_rate)\n",
    "    with Pool(num_processes) as p:\n",
    "        additional_features_list = list(tqdm(p.imap(worker, audio_list), total=len(audio_list), desc=\"Extracting additional features\"))\n",
    "    return additional_features_list\n",
    "\n",
    "# Load and process audio files from 'data' dataframe\n",
    "audio_files = data['processed_path'].tolist()  # Use paths from the pre-loaded 'data' dataframe\n",
    "\n",
    "audio_data = [load_and_resample_audio(file, target_sampling_rate) for file in tqdm(audio_files)]\n",
    "audio_list = [item[0] for item in audio_data]\n",
    "audio_lengths = [item[1] for item in audio_data]\n",
    "\n",
    "additional_features_list = extract_additional_features_parallel(audio_list, target_sampling_rate)\n",
    "\n",
    "additional_feature_columns = (\n",
    "    [f\"mfcc_{i}\" for i in range(1, 14)]\n",
    "    + [f\"chroma_{i}\" for i in range(1, 13)]\n",
    "    + [\"spectral_centroid\", \"spectral_rolloff\", \"zero_crossing_rate\", \"pitch\"]\n",
    ")\n",
    "additional_features_df = pd.DataFrame(additional_features_list, columns=additional_feature_columns)\n",
    "\n",
    "egemaps_model = audio_embeddings_model(\"egemaps\")\n",
    "print(\"Extracting eGeMAPS\")\n",
    "egemaps_features = audio_embeddings_parallel(audio_list, \"egemaps\", target_sampling_rate)\n",
    "wav2vec2_model_name = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "whisper_model_name = \"openai/whisper-medium\"\n",
    "print(\"Extracting Wav2Vec2 \")\n",
    "wav2vec2_features = extract_embeddings_from_transformer_model(audio_list, wav2vec2_model_name, target_sampling_rate)\n",
    "print(\"Extracting Whisper\")\n",
    "whisper_features = extract_whisper_embeddings(audio_list, whisper_model_name, target_sampling_rate)\n",
    "\n",
    "# Create DataFrame for all features\n",
    "features_df = pd.DataFrame(egemaps_features, columns=egemaps_model.column_names)\n",
    "features_df[\"Wav2Vec2_embeddings\"] = list(wav2vec2_features)\n",
    "features_df[\"Whisper_embeddings\"] = list(whisper_features)\n",
    "features_df[\"file_name\"] = [os.path.basename(file) for file in audio_files]\n",
    "features_df[\"augmentation_type\"] = \"original\"  # tag as original\n",
    "features_df = pd.concat([features_df, additional_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df['Wav2Vec2_embeddings'].tolist(), columns=[f'Embedding1_{i+1}' for i in range(len(features_df['Wav2Vec2_embeddings'][0]))])\n",
    "features_df_extracted = pd.concat([features_df.drop(columns=['Wav2Vec2_embeddings']), embeddings_features_df], axis=1)\n",
    "embeddings_features_df = pd.DataFrame(features_df_extracted['Whisper_embeddings'].tolist(), columns=[f'Embedding2_{i+1}' for i in range(len(features_df_extracted['Whisper_embeddings'][0]))])\n",
    "features_df_extracted = pd.concat([features_df_extracted.drop(columns=['Whisper_embeddings']), embeddings_features_df], axis=1)\n",
    "\n",
    "# Add the extracted features to the original 'data' dataframe\n",
    "# Assuming order is preserved, drop 'file_name' and 'augmentation_type' if not needed in 'data'\n",
    "data = pd.concat([data.reset_index(drop=True), features_df_extracted.drop(columns=['file_name', 'augmentation_type']).reset_index(drop=True)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"Full_data_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "siavash",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
