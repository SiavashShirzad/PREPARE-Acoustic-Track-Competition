{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirments.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "from mutagen.mp3 import MP3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testFeaturesPath = \"Features/test_audio_features.csv\"\n",
    "metadataPath = 'metadata.csv'\n",
    "testLabelPath = \"acoustic_test_labels.csv\"\n",
    "additionalMetadataPath = \"additional_metadata.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = joblib.load('Model/model.joblib')\n",
    "TestData = pd.read_csv(testFeaturesPath)\n",
    "TestData['uid'] = TestData['file_name'].apply(lambda x: x[:-4])\n",
    "meta = pd.read_csv(metadataPath)[['uid','age','gender']]\n",
    "meta['gender'] = meta['gender'].apply(lambda x : 1 if x == 'male' else 0)\n",
    "test_data = pd.merge(TestData, meta, on='uid')\n",
    "test_label = pd.read_csv(testLabelPath)\n",
    "test_label['label'] = (test_label['diagnosis_control'] + 2*test_label['diagnosis_mci'] + 3*test_label['diagnosis_adrd'] -1).astype(np.uint8)\n",
    "test_label.drop(['diagnosis_control'\t,'diagnosis_mci'\t,'diagnosis_adrd'], axis=1,inplace=True)\n",
    "test_data = pd.merge(test_data, test_label, on='uid')\n",
    "test_data.drop('augmentation_type', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uids = test_data['uid'].values\n",
    "labels = test_data['label'].values\n",
    "X_test = test_data.drop(columns=['uid', 'label', 'augmentation_type', 'file_name'], errors='ignore')\n",
    "y_test_proba = model.predict_proba(X_test)\n",
    "feature_names = X_test.columns.tolist()\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = label_encoder.fit_transform(labels)\n",
    "class_labels = label_encoder.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------\n",
    "# Evaluate on Test Set\n",
    "# ---------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_proba = model.predict_proba(X_test)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "test_metrics = {\n",
    "    'Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1-Score': [],\n",
    "    'Log Loss': []\n",
    "}\n",
    "\n",
    "per_class_test_metrics = {}\n",
    "for class_idx, class_label in enumerate(class_labels):\n",
    "    per_class_test_metrics[f'Class_{class_label}_Sensitivity'] = []\n",
    "    per_class_test_metrics[f'Class_{class_label}_Specificity'] = []\n",
    "\n",
    "acc = accuracy_score(labels, y_test_pred)\n",
    "precision = precision_score(labels, y_test_pred, average='weighted', zero_division=0)\n",
    "recall = recall_score(labels, y_test_pred, average='weighted', zero_division=0)\n",
    "f1 = f1_score(labels, y_test_pred, average='weighted', zero_division=0)\n",
    "loss = log_loss(labels, y_test_proba)\n",
    "\n",
    "test_metrics['Accuracy'].append(acc)\n",
    "test_metrics['Precision'].append(precision)\n",
    "test_metrics['Recall'].append(recall)\n",
    "test_metrics['F1-Score'].append(f1)\n",
    "test_metrics['Log Loss'].append(loss)\n",
    "\n",
    "cm = confusion_matrix(labels, y_test_pred, labels=range(len(class_labels)))\n",
    "\n",
    "for class_idx, class_label in enumerate(class_labels):\n",
    "    TP = cm[class_idx, class_idx]\n",
    "    FN = cm[class_idx, :].sum() - TP\n",
    "    FP = cm[:, class_idx].sum() - TP\n",
    "    TN = cm.sum() - (TP + FP + FN)\n",
    "\n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0.0\n",
    "\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0.0\n",
    "\n",
    "    per_class_test_metrics[f'Class_{class_label}_Sensitivity'].append(sensitivity)\n",
    "    per_class_test_metrics[f'Class_{class_label}_Specificity'].append(specificity)\n",
    "\n",
    "test_metrics_combined = {**test_metrics, **per_class_test_metrics}\n",
    "\n",
    "test_metrics_df = pd.DataFrame(test_metrics_combined)\n",
    "\n",
    "print(\"Test Set Metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"Log Loss: {loss:.4f}\")\n",
    "\n",
    "for class_label in class_labels:\n",
    "    sens = test_metrics_combined[f'Class_{class_label}_Sensitivity'][0]\n",
    "    spec = test_metrics_combined[f'Class_{class_label}_Specificity'][0]\n",
    "    print(f\"Class {class_label} Sensitivity: {sens:.4f}\")\n",
    "    print(f\"Class {class_label} Specificity: {spec:.4f}\")\n",
    "\n",
    "metrics_summary_test = pd.DataFrame({\n",
    "    'Metric': list(test_metrics_combined.keys()),\n",
    "    'Value': [test_metrics_combined[key][0] for key in test_metrics_combined]\n",
    "})\n",
    "\n",
    "metrics_summary_test.to_csv(\"test_validation_results.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_df = pd.DataFrame(\n",
    "    y_test_proba,\n",
    "    columns=['diagnosis_control', 'diagnosis_mci', 'diagnosis_adrd']\n",
    ")\n",
    "proba_df['uid'] = uids\n",
    "proba_df['confidence'] = proba_df[['diagnosis_control', 'diagnosis_mci', 'diagnosis_adrd']].max(axis=1)\n",
    "final_proba_df = proba_df.loc[proba_df.groupby('uid')['confidence'].idxmax()]\n",
    "final_proba_df = final_proba_df.drop(columns=['confidence'])\n",
    "final_proba_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "final_proba_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred = pd.DataFrame(\n",
    "    y_test_pred,\n",
    "    columns=['pred']\n",
    ")\n",
    "final_pred['uid'] = uids\n",
    "final_pred.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(additionalMetadataPath)\n",
    "mainMetadata = pd.read_csv(metadataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mp3_files_info(folder_path):\n",
    "    data = []\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.lower().endswith('.mp3'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            try:\n",
    "                audio = MP3(file_path)\n",
    "                length = audio.info.length \n",
    "                data.append({'uid': file_name, 'Length': length})\n",
    "            except Exception as e:\n",
    "                print(f\"Could not process file {file_name}: {e}\")\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "folder_path = '/home/siavash/Downloads/ACousticNIHCompetition/test_audios'\n",
    "mp3_info_df = get_mp3_files_info(folder_path)\n",
    "mp3_info_df['uid'] = mp3_info_df['uid'].apply(lambda x: x[:-4])\n",
    "mp3_info_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_analysis_dataset = pd.merge(final_pred, metadata, on='uid')\n",
    "bias_analysis_dataset = pd.merge(bias_analysis_dataset, test_label, on='uid')\n",
    "bias_analysis_dataset = pd.merge(bias_analysis_dataset, mainMetadata, on='uid')\n",
    "bias_analysis_dataset = pd.merge(bias_analysis_dataset, mp3_info_df, on='uid')\n",
    "bias_analysis_dataset = bias_analysis_dataset.drop(['diagnosis', 'split', 'filesize_kb', 'hash'], axis=1)\n",
    "bias_analysis_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2_contingency, ttest_ind\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "df = bias_analysis_dataset.copy()\n",
    "\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "\n",
    "categorical_cols = ['race', 'language', 'handedness', 'education', 'corpus', 'gender']\n",
    "numerical_cols = ['age', 'Length']\n",
    "\n",
    "for col in categorical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mode = df[col].mode()[0]\n",
    "        df[col].fillna(mode, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with mode: {mode}\")\n",
    "\n",
    "for col in numerical_cols:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median = df[col].median()\n",
    "        df[col].fillna(median, inplace=True)\n",
    "        print(f\"Filled missing values in '{col}' with median: {median}\")\n",
    "\n",
    "df['Correct'] = np.where(df['pred'] == df['label'], 'Correct', 'Incorrect')\n",
    "\n",
    "print(\"\\nPrediction Accuracy Distribution:\")\n",
    "print(df['Correct'].value_counts())\n",
    "\n",
    "def chi_square_test(df, column, alpha=0.05):\n",
    "    contingency_table = pd.crosstab(df[column], df['Correct'])\n",
    "    chi2, p, dof, ex = chi2_contingency(contingency_table)\n",
    "    result = {\n",
    "        'Variable': column,\n",
    "        'Chi2': chi2,\n",
    "        'p-value': p,\n",
    "        'Significant': p < alpha\n",
    "    }\n",
    "    return result\n",
    "\n",
    "chi2_results = []\n",
    "for col in categorical_cols:\n",
    "    result = chi_square_test(df, col)\n",
    "    chi2_results.append(result)\n",
    "\n",
    "chi2_df = pd.DataFrame(chi2_results)\n",
    "\n",
    "significant_chi2 = chi2_df[chi2_df['Significant']]\n",
    "\n",
    "print(\"\\nSignificant Chi-Square Test Results:\")\n",
    "print(significant_chi2[['Variable', 'Chi2', 'p-value']])\n",
    "\n",
    "def t_test(df, column, alpha=0.05):\n",
    "    group_correct = df[df['Correct'] == 'Correct'][column]\n",
    "    group_incorrect = df[df['Correct'] == 'Incorrect'][column]\n",
    "    t_stat, p = ttest_ind(group_correct, group_incorrect, equal_var=False)\n",
    "    result = {\n",
    "        'Variable': column,\n",
    "        't-Statistic': t_stat,\n",
    "        'p-value': p,\n",
    "        'Significant': p < alpha\n",
    "    }\n",
    "    return result\n",
    "\n",
    "ttest_results = []\n",
    "for col in numerical_cols:\n",
    "    result = t_test(df, col)\n",
    "    ttest_results.append(result)\n",
    "\n",
    "ttest_df = pd.DataFrame(ttest_results)\n",
    "\n",
    "significant_ttest = ttest_df[ttest_df['Significant']]\n",
    "\n",
    "print(\"\\nSignificant t-Test Results:\")\n",
    "print(significant_ttest[['Variable', 't-Statistic', 'p-value']])\n",
    "\n",
    "def plot_significant_categorical(df, column):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.countplot(data=df, x=column, hue='Correct')\n",
    "    plt.title(f'{column.capitalize()} vs Prediction Accuracy')\n",
    "    plt.legend(title='Prediction', loc='upper right')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for _, row in significant_chi2.iterrows():\n",
    "    plot_significant_categorical(df, row['Variable'])\n",
    "\n",
    "def plot_significant_numerical(df, column):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.boxplot(data=df, x='Correct', y=column)\n",
    "    plt.title(f'{column.capitalize()} Distribution by Prediction Accuracy')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "for _, row in significant_ttest.iterrows():\n",
    "    plot_significant_numerical(df, row['Variable'])\n",
    "\n",
    "def analyze_categorical(df, column):\n",
    "    contingency_table = pd.crosstab(df[column], df['Correct'], normalize='index') * 100\n",
    "    print(f\"\\nDetailed Analysis for '{column}':\")\n",
    "    print(contingency_table)\n",
    "    print(\"\\nInterpretation:\")\n",
    "    incorrect_rates = contingency_table['Incorrect']\n",
    "    highest_incorrect = incorrect_rates.idxmax()\n",
    "    highest_rate = incorrect_rates.max()\n",
    "    print(f\"- Category '{highest_incorrect}' in '{column}' has the highest incorrect prediction rate of {highest_rate:.2f}%.\")\n",
    "    print(f\"- Categories with lower incorrect rates may be contributing to higher accuracy.\")\n",
    "\n",
    "for _, row in significant_chi2.iterrows():\n",
    "    analyze_categorical(df, row['Variable'])\n",
    "\n",
    "def analyze_numerical(df, column):\n",
    "    group_correct = df[df['Correct'] == 'Correct'][column]\n",
    "    group_incorrect = df[df['Correct'] == 'Incorrect'][column]\n",
    "    mean_correct = group_correct.mean()\n",
    "    mean_incorrect = group_incorrect.mean()\n",
    "    print(f\"\\nDetailed Analysis for '{column}':\")\n",
    "    print(f\"- Mean {column} for Correct Predictions: {mean_correct:.2f}\")\n",
    "    print(f\"- Mean {column} for Incorrect Predictions: {mean_incorrect:.2f}\")\n",
    "    if mean_incorrect > mean_correct:\n",
    "        print(f\"--> Higher values of '{column}' are associated with incorrect predictions.\")\n",
    "    else:\n",
    "        print(f\"--> Lower values of '{column}' are associated with incorrect predictions.\")\n",
    "\n",
    "for _, row in significant_ttest.iterrows():\n",
    "    analyze_numerical(df, row['Variable'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_index = 5 \n",
    "\n",
    "X_single = X_test.iloc[[subject_index]].copy()\n",
    "\n",
    "selector = model.named_steps['select']\n",
    "xgb_model_final = model.named_steps['xgb']\n",
    "\n",
    "X_single_selected = selector.transform(X_single)\n",
    "\n",
    "selected_indices = selector.get_support(indices=True)\n",
    "selected_feature_names = [feature_names[i] for i in selected_indices]\n",
    "\n",
    "explainer = shap.TreeExplainer(xgb_model_final)\n",
    "\n",
    "\n",
    "shap_values_full = explainer.shap_values(X_single_selected)\n",
    "\n",
    "\n",
    "\n",
    "num_classes = shap_values_full.shape[2] \n",
    "class_names = list(label_encoder.classes_)  \n",
    "\n",
    "class_names_str = [str(cls) for cls in class_names]\n",
    "\n",
    "shap_values_list = [shap_values_full[:, :, c] for c in range(num_classes)]\n",
    "\n",
    "\n",
    "for c in range(num_classes):\n",
    "    print(f\"shap_values_list[{c}].shape =\", shap_values_list[c].shape)  \n",
    "\n",
    "\n",
    "def categorize_feature(feature_name):\n",
    "    if feature_name.startswith('Embedding1_'):\n",
    "        return 'Wav2Vec'\n",
    "    elif feature_name.startswith('Embedding2_'):\n",
    "        return 'Whisper'\n",
    "    elif 'age' in feature_name.lower():\n",
    "        return 'Age'\n",
    "    elif 'gender' in feature_name.lower():\n",
    "        return 'Gender'\n",
    "    else:\n",
    "        return 'OpenSMILE'\n",
    "\n",
    "feature_categories = [categorize_feature(fn) for fn in selected_feature_names]\n",
    "unique_categories = sorted(list(set(feature_categories)))\n",
    "print(\"Unique Categories:\", unique_categories)\n",
    "\n",
    "category_shap_sums = {cat: np.zeros(num_classes) for cat in unique_categories}\n",
    "\n",
    "for c in range(num_classes):\n",
    "    shap_values_for_c = shap_values_list[c]  \n",
    "    shap_1d = shap_values_for_c[0, :]      \n",
    "    \n",
    "    for feature_idx, cat in enumerate(feature_categories):\n",
    "        category_shap_sums[cat][c] += shap_1d[feature_idx]\n",
    "\n",
    "print(\"\\ncategory_shap_sums =\", category_shap_sums)\n",
    "\n",
    "\n",
    "df_category_shap = pd.DataFrame({\n",
    "    'Category': unique_categories\n",
    "})\n",
    "\n",
    "for c in range(num_classes):\n",
    "    df_category_shap[f'SHAP_{class_names_str[c]}'] = [\n",
    "        category_shap_sums[cat][c] for cat in unique_categories\n",
    "    ]\n",
    "\n",
    "print(\"\\n--- SHAP Summaries by Category for Subject Index:\", subject_index, \"---\")\n",
    "print(df_category_shap)\n",
    "\n",
    "\n",
    "category_palette = {\n",
    "    'Wav2Vec': '#1f77b4',   \n",
    "    'Whisper': '#ff7f0e',   \n",
    "    'OpenSMILE': '#2ca02c',  \n",
    "    'Age': '#9467bd',        \n",
    "    'Gender': '#8c564b'    \n",
    "}\n",
    "\n",
    "\n",
    "for cat in unique_categories:\n",
    "    if cat not in category_palette:\n",
    "        category_palette[cat] = '#7f7f7f'  \n",
    "\n",
    "for c in range(num_classes):\n",
    "    class_label = class_names_str[c]  \n",
    "    plt.figure(figsize=(10, 6))  \n",
    "    sns.barplot(\n",
    "        x='SHAP_' + class_label,\n",
    "        y='Category',\n",
    "        data=df_category_shap,\n",
    "        order=unique_categories,\n",
    "        palette=[category_palette[cat] for cat in unique_categories]\n",
    "    )\n",
    "    plt.title(f\"Subject {subject_index}: SHAP by Category for Class = {class_label}\", fontsize=18)\n",
    "    plt.xlabel(\"SHAP Contribution\", fontsize=14)\n",
    "    plt.ylabel(\"Category\", fontsize=14)\n",
    "    plt.axvline(0, color='grey', linewidth=0.8)  \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
